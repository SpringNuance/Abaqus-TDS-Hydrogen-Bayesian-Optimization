{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EJfeFpYUCmd6"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXf24hQtCmeM",
    "outputId": "d36a15be-963a-4db6-daf2-cd68e09409b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is: \n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project/notebooks/CP1000_RD_20C\n",
      "The current directory is: \n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "from math import *\n",
    "\n",
    "print(\"The current directory is: \")\n",
    "print(os.getcwd())\n",
    "if not os.getcwd().endswith(\"Abaqus-Hardening-Seq-2-Seq-Project\"):\n",
    "    # Move up two directories\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)\n",
    "print(\"The current directory is: \")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uK4hhp8dCmeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "= Stage 1: Loading configs and all paths =\n",
      "==========================================\n",
      "\n",
      "Welcome to Abaqus Seq2Seq flow curve calibration project\n",
      "\n",
      "The configurations you have chosen: \n",
      "\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "|      Global Configs      |                         User choice                         |\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "|         PROJECT          |                        CP1000_RD_20C                        |\n",
      "|        OBJECTIVES        |      CHD2, CHD4, NDBR2p5, NDBR6, NDBR15, NDBR40, SH115      |\n",
      "|       PROJECT_PATH       | /scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project |\n",
      "|    TRAINING_DATA_PATH    |                 training_data/CP1000_RD_20C                 |\n",
      "|         LOG_PATH         |                    log/CP1000_RD_20C.txt                    |\n",
      "|       MODELS_PATH        |                     models/CP1000_RD_20C                    |\n",
      "|  RESULTS_INIT_DATA_PATH  |              results_initial_data/CP1000_RD_20C             |\n",
      "| RESULTS_INIT_COMMON_PATH |             results_initial_common/CP1000_RD_20C            |\n",
      "|  RESULTS_ITER_DATA_PATH  |             results_iteration_data/CP1000_RD_20C            |\n",
      "| RESULTS_ITER_COMMON_PATH |            results_iteration_common/CP1000_RD_20C           |\n",
      "|       SCRIPTS_PATH       |                    scripts/CP1000_RD_20C                    |\n",
      "|      SIMS_INIT_PATH      |                  sims_initial/CP1000_RD_20C                 |\n",
      "|      SIMS_ITER_PATH      |                 sims_iteration/CP1000_RD_20C                |\n",
      "|       TARGETS_PATH       |                    targets/CP1000_RD_20C                    |\n",
      "|      TEMPLATES_PATH      |                   templates/CP1000_RD_20C                   |\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "\n",
      "The path to your main project folder is\n",
      "\n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.chosen_project import *\n",
    "from src.stage1_global_configs import *\n",
    "\n",
    "chosen_project_path = \"configs/global_config_CP1000_RD_20C.json\"\n",
    "\n",
    "global_configs = main_global_configs(chosen_project_path)\n",
    "\n",
    "all_paths = global_configs['all_paths']\n",
    "objectives = global_configs['objectives']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIYPPhgbCmeP"
   },
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4IiEAFmCmeR",
    "outputId": "88b0fd59-7d9e-42e0-9a43-5723a8b54351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training source sequence: torch.Size([192, 100, 7])\n",
      "Shape of the training target sequence: torch.Size([192, 1, 1])\n",
      "Shape of the testing source sequence: torch.Size([64, 100, 7])\n",
      "Shape of the testing target sequence: torch.Size([64, 1, 1])\n",
      "Number of NaN values in train_source_sequence: 0\n",
      "Number of NaN values in train_target_sequence: 0\n",
      "Number of NaN values in test_source_sequence: 0\n",
      "Number of NaN values in test_target_sequence: 0\n",
      "Number of infinite values in train_source_sequence: 0\n",
      "Number of infinite values in train_target_sequence: 0\n",
      "Number of infinite values in test_source_sequence: 0\n",
      "Number of infinite values in test_target_sequence: 0\n",
      "Number of negative values in train_target_sequence: 0\n",
      "Number of negative values in test_target_sequence: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "training_data_path = all_paths['training_data_path']\n",
    "models_path = all_paths['models_path']\n",
    "\n",
    "initial_train_source_sequence = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_train_source_original_all.pt\")\n",
    "initial_train_target_sequence_first = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_train_target_original_first.pt\")\n",
    "initial_test_source_sequence = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_test_source_original_all.pt\")\n",
    "initial_test_target_sequence_first = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_test_target_original_first.pt\")\n",
    "\n",
    "# Convert them to float32\n",
    "\n",
    "initial_train_source_sequence = initial_train_source_sequence.float()\n",
    "initial_train_target_sequence_first = initial_train_target_sequence_first.float()\n",
    "initial_test_source_sequence = initial_test_source_sequence.float()\n",
    "initial_test_target_sequence_first = initial_test_target_sequence_first.float()\n",
    "\n",
    "print(f\"Shape of the training source sequence: {initial_train_source_sequence.shape}\")\n",
    "print(f\"Shape of the training target sequence: {initial_train_target_sequence_first.shape}\")\n",
    "print(f\"Shape of the testing source sequence: {initial_test_source_sequence.shape}\")\n",
    "print(f\"Shape of the testing target sequence: {initial_test_target_sequence_first.shape}\")\n",
    "\n",
    "# Check if any of them has NaN or infinite values\n",
    "\n",
    "print(f\"Number of NaN values in train_source_sequence: {np.isnan(initial_train_source_sequence).sum()}\")\n",
    "print(f\"Number of NaN values in train_target_sequence: {np.isnan(initial_train_target_sequence_first).sum()}\")\n",
    "print(f\"Number of NaN values in test_source_sequence: {np.isnan(initial_test_source_sequence).sum()}\")\n",
    "print(f\"Number of NaN values in test_target_sequence: {np.isnan(initial_test_target_sequence_first).sum()}\")\n",
    "\n",
    "print(f\"Number of infinite values in train_source_sequence: {np.isinf(initial_train_source_sequence).sum()}\")\n",
    "print(f\"Number of infinite values in train_target_sequence: {np.isinf(initial_train_target_sequence_first).sum()}\")\n",
    "print(f\"Number of infinite values in test_source_sequence: {np.isinf(initial_test_source_sequence).sum()}\")\n",
    "print(f\"Number of infinite values in test_target_sequence: {np.isinf(initial_test_target_sequence_first).sum()}\")\n",
    "\n",
    "# Ensure that all target_sequence are positive\n",
    "print(f\"Number of negative values in train_target_sequence: {(initial_train_target_sequence_first < 0).sum()}\")\n",
    "print(f\"Number of negative values in test_target_sequence: {(initial_test_target_sequence_first < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.5942,  9.5506,  9.6752,  9.4244,  9.1070,  8.7356,  1.3993])\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the scale of the source sequence is correct\n",
    "print(initial_train_source_sequence[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([807.0139])\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the scale of the target sequence is correct\n",
    "print(initial_train_target_sequence_first[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jh43TFa16Xjx",
    "outputId": "2ac298f1-00e2-48cc-c53d-4ede2b78834b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "\n",
    "training_env = \"CSC\" # Choose between \"local\" and \"CSC\"\n",
    "if training_env == \"local\":\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE used in Transformer does not need to be weighted like the case in training bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization.transformer_helper import *\n",
    "from optimization.transformer import *\n",
    "\n",
    "def train_transformer(previous_model, current_model, previous_best_test_loss, num_epochs,\n",
    "                       dropout, learning_rate, weight_decay):\n",
    "    \n",
    "    train_dataset = TensorDataset(initial_train_source_sequence, initial_train_target_sequence_first)\n",
    "    test_dataset = TensorDataset(initial_test_source_sequence, initial_test_target_sequence_first)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Parameters\n",
    "    _, source_len, feature_size = initial_train_source_sequence.shape\n",
    "    _, label_size, _ = initial_train_target_sequence_first.shape\n",
    "    \n",
    "    d_model = 256\n",
    "    n_heads = 16 # rule of thumb: d_model/n_heads = 16 or 32\n",
    "    num_layers = 4\n",
    "    dim_feedforward = 1024\n",
    "    # a larger feedforward dimension (often 2-4 times d_model) helps the model to process \n",
    "    # and transform the information more effectively\n",
    "    \n",
    "    activation_name = \"relu\" # \"relu\" or \"gelu\"\n",
    "    pos_enc_type=\"fixed\" # \"fixed\" or \"learnable\"\n",
    "    encoder_layer_type=\"LayerNorm\" # \"LayerNorm\" or \"BatchNorm\"\n",
    "        \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    \n",
    "    model = TransformerEncoder(feature_size, label_size, source_len,\n",
    "                     d_model, n_heads, num_layers, dim_feedforward, \n",
    "                     activation_name, pos_enc_type, encoder_layer_type,\n",
    "                     dropout=dropout).to(device)\n",
    "    \n",
    "    criterion = RMSELoss() \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Adding L2 regularization\n",
    "    \n",
    "    # Count the number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'The model has {total_params} parameters.')\n",
    "    \n",
    "    if previous_model is not None:\n",
    "        # Loading the best model from the previous training\n",
    "        model.load_state_dict(torch.load(f\"{models_path}/transformer/initial/best_model_{previous_model}.pth\"))\n",
    "        best_test_loss = previous_best_test_loss\n",
    "    else:\n",
    "        best_test_loss = float('inf')\n",
    "        \n",
    "    # Track the best model\n",
    "    best_model_path = f\"{models_path}/transformer/initial/best_model_{current_model}.pth\"\n",
    "\n",
    "    # Lists to track train and test losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "            \n",
    "        for batch_idx, (source_batch, target_batch) in enumerate(train_loader):\n",
    "            source_batch, target_batch = source_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(source_batch)\n",
    "            loss = criterion(outputs, target_batch)\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for source_batch, target_batch in test_loader:\n",
    "                source_batch, target_batch = source_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(source_batch)\n",
    "                loss = criterion(outputs, target_batch)\n",
    "    \n",
    "                test_loss += loss.item()\n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Save the best model\n",
    "        if test_loss < best_test_loss:\n",
    "            print(f\"New best test loss found: {test_loss}\")\n",
    "            best_test_loss = test_loss\n",
    "            best_model = model\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "        # Print progress\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.8f}, Test Loss: {test_loss:.8f}')\n",
    "    \n",
    "    # Save the train and test loss lists as .npy files\n",
    "    np.save(f'{models_path}/transformer/initial/train_losses_{current_model}.npy', np.array(train_losses))\n",
    "    np.save(f'{models_path}/transformer/initial/test_losses_{current_model}.npy', np.array(test_losses))\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"{models_path}/transformer/initial/last_model_{current_model}.pth\")\n",
    "    \n",
    "    print('Training complete')\n",
    "    print(f'Best model saved with test loss: {best_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3186689 parameters.\n",
      "New best test loss found: 869.542236328125\n",
      "New best test loss found: 862.4310913085938\n",
      "New best test loss found: 855.6124267578125\n",
      "New best test loss found: 848.6881713867188\n",
      "New best test loss found: 841.6080322265625\n",
      "New best test loss found: 834.3837280273438\n",
      "New best test loss found: 827.0298461914062\n",
      "New best test loss found: 819.55859375\n",
      "New best test loss found: 811.9977416992188\n",
      "New best test loss found: 804.3422241210938\n",
      "New best test loss found: 796.5868530273438\n",
      "New best test loss found: 788.7526245117188\n",
      "New best test loss found: 780.8709106445312\n",
      "New best test loss found: 772.9435424804688\n",
      "New best test loss found: 764.95751953125\n",
      "New best test loss found: 756.939453125\n",
      "New best test loss found: 748.8925170898438\n",
      "New best test loss found: 740.811279296875\n",
      "New best test loss found: 732.7067260742188\n",
      "New best test loss found: 724.58154296875\n",
      "New best test loss found: 716.4357299804688\n",
      "New best test loss found: 708.272705078125\n",
      "New best test loss found: 700.0972900390625\n",
      "New best test loss found: 691.899169921875\n",
      "New best test loss found: 683.6982421875\n",
      "New best test loss found: 675.4697265625\n",
      "New best test loss found: 667.2349853515625\n",
      "New best test loss found: 658.994873046875\n",
      "New best test loss found: 650.7307739257812\n",
      "New best test loss found: 642.4591064453125\n",
      "New best test loss found: 634.1895751953125\n",
      "New best test loss found: 625.8873901367188\n",
      "New best test loss found: 617.6072998046875\n",
      "New best test loss found: 609.2897338867188\n",
      "New best test loss found: 600.9630126953125\n",
      "New best test loss found: 592.6477661132812\n",
      "New best test loss found: 584.290771484375\n",
      "New best test loss found: 575.9443359375\n",
      "New best test loss found: 567.595703125\n",
      "New best test loss found: 559.2284545898438\n",
      "New best test loss found: 550.8847045898438\n",
      "New best test loss found: 542.5328369140625\n",
      "New best test loss found: 534.1187133789062\n",
      "New best test loss found: 525.7222290039062\n",
      "New best test loss found: 517.31982421875\n",
      "New best test loss found: 508.912841796875\n",
      "New best test loss found: 500.5013122558594\n",
      "New best test loss found: 492.0872802734375\n",
      "New best test loss found: 483.67449951171875\n",
      "New best test loss found: 475.261962890625\n",
      "New best test loss found: 466.81634521484375\n",
      "New best test loss found: 458.3747253417969\n",
      "New best test loss found: 449.9350280761719\n",
      "New best test loss found: 441.50244140625\n",
      "New best test loss found: 433.080078125\n",
      "New best test loss found: 424.618408203125\n",
      "New best test loss found: 416.17901611328125\n",
      "New best test loss found: 407.7321472167969\n",
      "New best test loss found: 399.2889099121094\n",
      "New best test loss found: 390.8399658203125\n",
      "New best test loss found: 382.4052429199219\n",
      "New best test loss found: 374.00421142578125\n",
      "New best test loss found: 365.5944519042969\n",
      "New best test loss found: 357.1421813964844\n",
      "New best test loss found: 348.7236633300781\n",
      "New best test loss found: 340.3297424316406\n",
      "New best test loss found: 331.94091796875\n",
      "New best test loss found: 323.58843994140625\n",
      "New best test loss found: 315.24835205078125\n",
      "New best test loss found: 306.90655517578125\n",
      "New best test loss found: 298.5821228027344\n",
      "New best test loss found: 290.29437255859375\n",
      "New best test loss found: 282.0487976074219\n",
      "New best test loss found: 273.81134033203125\n",
      "New best test loss found: 265.641357421875\n",
      "New best test loss found: 257.50762939453125\n",
      "New best test loss found: 249.39906311035156\n",
      "New best test loss found: 241.3855743408203\n",
      "New best test loss found: 233.41830444335938\n",
      "New best test loss found: 225.5122833251953\n",
      "New best test loss found: 217.6895751953125\n",
      "New best test loss found: 209.9545440673828\n",
      "New best test loss found: 202.30238342285156\n",
      "New best test loss found: 194.80874633789062\n",
      "New best test loss found: 187.3918914794922\n",
      "New best test loss found: 180.13429260253906\n",
      "New best test loss found: 173.04457092285156\n",
      "New best test loss found: 166.13182067871094\n",
      "New best test loss found: 159.4263458251953\n",
      "New best test loss found: 152.94435119628906\n",
      "New best test loss found: 146.71681213378906\n",
      "New best test loss found: 140.7608642578125\n",
      "New best test loss found: 135.09530639648438\n",
      "New best test loss found: 129.7607421875\n",
      "New best test loss found: 124.77677917480469\n",
      "New best test loss found: 120.2330093383789\n",
      "New best test loss found: 116.06639862060547\n",
      "New best test loss found: 112.26869201660156\n",
      "New best test loss found: 108.94327545166016\n",
      "New best test loss found: 106.08175659179688\n",
      "Epoch [100/1000], Train Loss: 107.69783020, Test Loss: 106.08175659\n",
      "New best test loss found: 103.5889663696289\n",
      "New best test loss found: 101.50077056884766\n",
      "New best test loss found: 99.81307220458984\n",
      "New best test loss found: 98.42127990722656\n",
      "New best test loss found: 97.3302001953125\n",
      "New best test loss found: 96.49664306640625\n",
      "New best test loss found: 95.87146759033203\n",
      "New best test loss found: 95.31067657470703\n",
      "New best test loss found: 94.90342712402344\n",
      "New best test loss found: 94.84141540527344\n",
      "New best test loss found: 94.54859924316406\n",
      "New best test loss found: 94.06869506835938\n",
      "New best test loss found: 93.87039184570312\n",
      "New best test loss found: 92.11157989501953\n",
      "New best test loss found: 90.40228271484375\n",
      "New best test loss found: 89.66374206542969\n",
      "New best test loss found: 86.98788452148438\n",
      "New best test loss found: 84.32315826416016\n",
      "New best test loss found: 81.84249877929688\n",
      "New best test loss found: 63.89901351928711\n",
      "New best test loss found: 60.807037353515625\n",
      "New best test loss found: 59.503780364990234\n",
      "New best test loss found: 57.81431198120117\n",
      "New best test loss found: 53.20769119262695\n",
      "New best test loss found: 51.297218322753906\n",
      "New best test loss found: 48.5798225402832\n",
      "New best test loss found: 45.98632049560547\n",
      "New best test loss found: 40.41081237792969\n",
      "New best test loss found: 39.61574935913086\n",
      "New best test loss found: 37.21967315673828\n",
      "New best test loss found: 36.13405227661133\n",
      "New best test loss found: 35.90549087524414\n",
      "New best test loss found: 35.36798095703125\n",
      "New best test loss found: 35.337547302246094\n",
      "New best test loss found: 32.52875518798828\n",
      "New best test loss found: 30.802656173706055\n",
      "New best test loss found: 29.883380889892578\n",
      "Epoch [200/1000], Train Loss: 47.87773895, Test Loss: 37.18707275\n",
      "New best test loss found: 28.842811584472656\n",
      "New best test loss found: 27.431150436401367\n",
      "New best test loss found: 27.139930725097656\n",
      "New best test loss found: 26.188169479370117\n",
      "New best test loss found: 25.68712043762207\n",
      "New best test loss found: 25.61862564086914\n",
      "New best test loss found: 25.026208877563477\n",
      "New best test loss found: 24.351734161376953\n",
      "New best test loss found: 23.653453826904297\n",
      "New best test loss found: 23.584196090698242\n",
      "New best test loss found: 22.461490631103516\n",
      "New best test loss found: 21.81808853149414\n",
      "New best test loss found: 21.6788272857666\n",
      "New best test loss found: 20.54273223876953\n",
      "New best test loss found: 20.101642608642578\n",
      "New best test loss found: 20.078054428100586\n",
      "New best test loss found: 17.802478790283203\n",
      "New best test loss found: 16.640460968017578\n",
      "New best test loss found: 16.562198638916016\n",
      "New best test loss found: 16.336605072021484\n",
      "New best test loss found: 15.094894409179688\n",
      "New best test loss found: 14.182890892028809\n",
      "New best test loss found: 13.916365623474121\n",
      "New best test loss found: 13.512584686279297\n",
      "New best test loss found: 13.225465774536133\n",
      "Epoch [300/1000], Train Loss: 14.51267846, Test Loss: 14.59784794\n",
      "New best test loss found: 12.761719703674316\n",
      "New best test loss found: 12.572766304016113\n",
      "New best test loss found: 11.923805236816406\n",
      "New best test loss found: 11.147469520568848\n",
      "New best test loss found: 11.143510818481445\n",
      "New best test loss found: 10.39082145690918\n",
      "New best test loss found: 9.554545402526855\n",
      "Epoch [400/1000], Train Loss: 9.83002218, Test Loss: 11.31377888\n",
      "New best test loss found: 9.3078031539917\n",
      "New best test loss found: 9.291647911071777\n",
      "New best test loss found: 8.816649436950684\n",
      "New best test loss found: 8.5463228225708\n",
      "Epoch [500/1000], Train Loss: 14.89157836, Test Loss: 17.32789803\n",
      "New best test loss found: 8.50281810760498\n",
      "New best test loss found: 8.382050514221191\n",
      "Epoch [600/1000], Train Loss: 8.92460442, Test Loss: 8.83238602\n",
      "New best test loss found: 8.01860237121582\n",
      "New best test loss found: 7.967559337615967\n",
      "New best test loss found: 7.835533618927002\n",
      "New best test loss found: 7.773841857910156\n",
      "Epoch [700/1000], Train Loss: 9.93438435, Test Loss: 9.32837582\n",
      "New best test loss found: 7.656185626983643\n",
      "New best test loss found: 7.5987067222595215\n",
      "New best test loss found: 7.582861423492432\n",
      "New best test loss found: 7.5534868240356445\n",
      "New best test loss found: 7.540496826171875\n",
      "New best test loss found: 7.381001949310303\n",
      "New best test loss found: 7.292358875274658\n",
      "Epoch [800/1000], Train Loss: 9.27140808, Test Loss: 8.25583553\n",
      "New best test loss found: 7.158210754394531\n",
      "New best test loss found: 7.135352611541748\n",
      "New best test loss found: 7.0766119956970215\n",
      "New best test loss found: 6.96976900100708\n",
      "New best test loss found: 6.913049221038818\n",
      "New best test loss found: 6.866804122924805\n",
      "Epoch [900/1000], Train Loss: 8.86335580, Test Loss: 8.71471500\n",
      "New best test loss found: 6.840307712554932\n",
      "New best test loss found: 6.7293171882629395\n",
      "Epoch [1000/1000], Train Loss: 11.27351173, Test Loss: 8.00228882\n",
      "Training complete\n",
      "Best model saved with test loss: 6.7293\n"
     ]
    }
   ],
   "source": [
    "train_transformer(previous_model=None, current_model=\"1_train_original\", previous_best_test_loss=None, \n",
    "                  num_epochs=1000, dropout=0.01, learning_rate=0.0001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3186689 parameters.\n",
      "New best test loss found: 6.645035743713379\n",
      "New best test loss found: 6.617631435394287\n",
      "New best test loss found: 6.571237564086914\n",
      "New best test loss found: 6.53059196472168\n",
      "Epoch [100/1000], Train Loss: 6.04089912, Test Loss: 6.88534689\n",
      "New best test loss found: 6.502954483032227\n",
      "New best test loss found: 6.458877086639404\n",
      "New best test loss found: 6.442684650421143\n",
      "New best test loss found: 6.428459644317627\n",
      "New best test loss found: 6.391922473907471\n",
      "Epoch [200/1000], Train Loss: 5.67611535, Test Loss: 6.39192247\n",
      "New best test loss found: 6.382574081420898\n",
      "New best test loss found: 6.374117374420166\n",
      "New best test loss found: 6.349586009979248\n",
      "New best test loss found: 6.334413051605225\n",
      "New best test loss found: 6.279034614562988\n",
      "Epoch [300/1000], Train Loss: 6.37706168, Test Loss: 6.40292931\n",
      "New best test loss found: 6.228068828582764\n",
      "Epoch [400/1000], Train Loss: 6.95079374, Test Loss: 7.30788994\n",
      "New best test loss found: 6.136126518249512\n",
      "New best test loss found: 6.095792293548584\n",
      "Epoch [500/1000], Train Loss: 5.81010278, Test Loss: 6.39009714\n",
      "New best test loss found: 6.079285144805908\n",
      "New best test loss found: 6.064095973968506\n",
      "Epoch [600/1000], Train Loss: 5.66670911, Test Loss: 6.33385754\n",
      "New best test loss found: 6.040727138519287\n",
      "New best test loss found: 6.030401706695557\n",
      "Epoch [700/1000], Train Loss: 5.22166395, Test Loss: 6.08873606\n",
      "New best test loss found: 6.021963119506836\n",
      "New best test loss found: 5.966175556182861\n",
      "Epoch [800/1000], Train Loss: 5.53605954, Test Loss: 6.78229618\n",
      "Epoch [900/1000], Train Loss: 5.39129384, Test Loss: 6.11342812\n",
      "New best test loss found: 5.9606032371521\n",
      "Epoch [1000/1000], Train Loss: 5.56885862, Test Loss: 7.13979816\n",
      "Training complete\n",
      "Best model saved with test loss: 5.9606\n"
     ]
    }
   ],
   "source": [
    "train_transformer(previous_model=\"1_train_original\", current_model=\"2_train_original\", previous_best_test_loss=6.7293171882629395, \n",
    "                  num_epochs=1000, dropout=0.01, learning_rate=0.00001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3186689 parameters.\n",
      "New best test loss found: 5.9406280517578125\n",
      "Epoch [100/1000], Train Loss: 5.62165165, Test Loss: 7.15306950\n",
      "Epoch [200/1000], Train Loss: 5.20612129, Test Loss: 6.56539202\n",
      "New best test loss found: 5.932235240936279\n",
      "Epoch [300/1000], Train Loss: 5.47065004, Test Loss: 6.19735193\n",
      "Epoch [400/1000], Train Loss: 5.38592275, Test Loss: 5.97682095\n",
      "Epoch [500/1000], Train Loss: 5.10699113, Test Loss: 6.13023663\n",
      "New best test loss found: 5.930030345916748\n",
      "New best test loss found: 5.874922752380371\n",
      "New best test loss found: 5.863852500915527\n",
      "Epoch [600/1000], Train Loss: 5.16608810, Test Loss: 6.53319931\n",
      "New best test loss found: 5.848926067352295\n",
      "Epoch [700/1000], Train Loss: 5.52058411, Test Loss: 6.24155807\n",
      "Epoch [800/1000], Train Loss: 5.56987302, Test Loss: 6.05510569\n",
      "Epoch [900/1000], Train Loss: 4.81442587, Test Loss: 6.46617746\n",
      "Epoch [1000/1000], Train Loss: 4.77395010, Test Loss: 6.60230255\n",
      "Training complete\n",
      "Best model saved with test loss: 5.8489\n"
     ]
    }
   ],
   "source": [
    "train_transformer(previous_model=\"2_train_original\", current_model=\"3_train_original\", previous_best_test_loss=5.9606032371521, \n",
    "                  num_epochs=1000, dropout=0.01, learning_rate=0.000005, weight_decay=1e-3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5092767,
     "sourceId": 8527979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
