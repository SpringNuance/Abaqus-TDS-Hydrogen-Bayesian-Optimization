{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJfeFpYUCmd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXf24hQtCmeM",
    "outputId": "d36a15be-963a-4db6-daf2-cd68e09409b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is: \n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project\n",
      "The current directory is: \n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "from math import *\n",
    "\n",
    "print(\"The current directory is: \")\n",
    "print(os.getcwd())\n",
    "if not os.getcwd().endswith(\"Abaqus-Hardening-Seq-2-Seq-Project\"):\n",
    "    # Move up two directories\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)\n",
    "print(\"The current directory is: \")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uK4hhp8dCmeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "= Stage 1: Loading configs and all paths =\n",
      "==========================================\n",
      "\n",
      "Welcome to Abaqus Seq2Seq flow curve calibration project\n",
      "\n",
      "The configurations you have chosen: \n",
      "\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "|      Global Configs      |                         User choice                         |\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "|         PROJECT          |                        CP1000_RD_20C                        |\n",
      "|        OBJECTIVES        |      CHD2, CHD4, NDBR2p5, NDBR6, NDBR15, NDBR40, SH115      |\n",
      "|       PROJECT_PATH       | /scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project |\n",
      "|    TRAINING_DATA_PATH    |                 training_data/CP1000_RD_20C                 |\n",
      "|         LOG_PATH         |                    log/CP1000_RD_20C.txt                    |\n",
      "|       MODELS_PATH        |                     models/CP1000_RD_20C                    |\n",
      "|  RESULTS_INIT_DATA_PATH  |              results_initial_data/CP1000_RD_20C             |\n",
      "| RESULTS_INIT_COMMON_PATH |             results_initial_common/CP1000_RD_20C            |\n",
      "|  RESULTS_ITER_DATA_PATH  |             results_iteration_data/CP1000_RD_20C            |\n",
      "| RESULTS_ITER_COMMON_PATH |            results_iteration_common/CP1000_RD_20C           |\n",
      "|       SCRIPTS_PATH       |                    scripts/CP1000_RD_20C                    |\n",
      "|      SIMS_INIT_PATH      |                  sims_initial/CP1000_RD_20C                 |\n",
      "|      SIMS_ITER_PATH      |                 sims_iteration/CP1000_RD_20C                |\n",
      "|       TARGETS_PATH       |                    targets/CP1000_RD_20C                    |\n",
      "|      TEMPLATES_PATH      |                   templates/CP1000_RD_20C                   |\n",
      "+--------------------------+-------------------------------------------------------------+\n",
      "\n",
      "The path to your main project folder is\n",
      "\n",
      "/scratch/project_2008630/Abaqus-Hardening-Seq-2-Seq-Project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.chosen_project import *\n",
    "from src.stage1_global_configs import *\n",
    "\n",
    "chosen_project_path = \"configs/global_config_CP1000_RD_20C.json\"\n",
    "\n",
    "global_configs = main_global_configs(chosen_project_path)\n",
    "\n",
    "all_paths = global_configs['all_paths']\n",
    "objectives = global_configs['objectives']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIYPPhgbCmeP"
   },
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4IiEAFmCmeR",
    "outputId": "88b0fd59-7d9e-42e0-9a43-5723a8b54351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training source sequence: torch.Size([192, 99, 7])\n",
      "Shape of the training target sequence: torch.Size([192, 99, 1])\n",
      "Shape of the testing source sequence: torch.Size([64, 99, 7])\n",
      "Shape of the testing target sequence: torch.Size([64, 99, 1])\n",
      "Number of NaN values in train_source_sequence: 0\n",
      "Number of NaN values in train_target_sequence: 0\n",
      "Number of NaN values in test_source_sequence: 0\n",
      "Number of NaN values in test_target_sequence: 0\n",
      "Number of infinite values in train_source_sequence: 0\n",
      "Number of infinite values in train_target_sequence: 0\n",
      "Number of infinite values in test_source_sequence: 0\n",
      "Number of infinite values in test_target_sequence: 0\n",
      "Number of negative values in train_target_sequence: 0\n",
      "Number of negative values in test_target_sequence: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "training_data_path = all_paths['training_data_path']\n",
    "models_path = all_paths['models_path']\n",
    "\n",
    "initial_train_source_sequence_diff = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_train_source_diff_all.pt\")\n",
    "initial_train_target_sequence_diff = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0/initial_train_target_diff_last.pt\")\n",
    "\n",
    "initial_test_source_sequence_diff = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0//initial_test_source_diff_all.pt\")\n",
    "initial_test_target_sequence_diff = torch.load(f\"training_data/CP1000_RD_20C_divided_index_0//initial_test_target_diff_last.pt\")\n",
    "\n",
    "# Convert them to float32\n",
    "\n",
    "initial_train_source_sequence_diff = initial_train_source_sequence_diff.float()\n",
    "initial_train_target_sequence_diff = initial_train_target_sequence_diff.float()\n",
    "initial_test_source_sequence_diff = initial_test_source_sequence_diff.float()\n",
    "initial_test_target_sequence_diff = initial_test_target_sequence_diff.float()\n",
    "\n",
    "print(f\"Shape of the training source sequence: {initial_train_source_sequence_diff.shape}\")\n",
    "print(f\"Shape of the training target sequence: {initial_train_target_sequence_diff.shape}\")\n",
    "print(f\"Shape of the testing source sequence: {initial_test_source_sequence_diff.shape}\")\n",
    "print(f\"Shape of the testing target sequence: {initial_test_target_sequence_diff.shape}\")\n",
    "\n",
    "# Check if any of them has NaN or infinite values\n",
    "\n",
    "print(f\"Number of NaN values in train_source_sequence: {np.isnan(initial_train_source_sequence_diff).sum()}\")\n",
    "print(f\"Number of NaN values in train_target_sequence: {np.isnan(initial_train_target_sequence_diff).sum()}\")\n",
    "print(f\"Number of NaN values in test_source_sequence: {np.isnan(initial_test_source_sequence_diff).sum()}\")\n",
    "print(f\"Number of NaN values in test_target_sequence: {np.isnan(initial_test_target_sequence_diff).sum()}\")\n",
    "\n",
    "print(f\"Number of infinite values in train_source_sequence: {np.isinf(initial_train_source_sequence_diff).sum()}\")\n",
    "print(f\"Number of infinite values in train_target_sequence: {np.isinf(initial_train_target_sequence_diff).sum()}\")\n",
    "print(f\"Number of infinite values in test_source_sequence: {np.isinf(initial_test_source_sequence_diff).sum()}\")\n",
    "print(f\"Number of infinite values in test_target_sequence: {np.isinf(initial_test_target_sequence_diff).sum()}\")\n",
    "\n",
    "# Ensure that all target_sequence are positive\n",
    "print(f\"Number of negative values in train_target_sequence: {(initial_train_target_sequence_diff < 0).sum()}\")\n",
    "print(f\"Number of negative values in test_target_sequence: {(initial_test_target_sequence_diff < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0443, 0.0462, 0.0313, 0.0378, 0.0508, 0.0566, 0.0325])\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the scale of the source sequence is correct\n",
    "print(initial_train_source_sequence_diff[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4783])\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the scale of the target sequence is correct\n",
    "print(initial_train_target_sequence_diff[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jh43TFa16Xjx",
    "outputId": "2ac298f1-00e2-48cc-c53d-4ede2b78834b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "\n",
    "training_env = \"CSC\" # Choose between \"local\" and \"CSC\"\n",
    "if training_env == \"local\":\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99])\n",
      "tensor(1.2323)\n",
      "tensor(0.7763)\n",
      "torch.Size([99])\n",
      "tensor(1.2348)\n",
      "tensor(0.7847)\n",
      "Training Data:\n",
      "Ratio First to Last Timestep: tensor(1.5873)\n",
      "\n",
      "Test Data:\n",
      "Ratio First to Last Timestep: tensor(1.5735)\n"
     ]
    }
   ],
   "source": [
    "# Function to process the data\n",
    "def process_sequence(tensor):\n",
    "    # Calculate the minimum across the batch size dimension\n",
    "    mean_values = tensor.mean(dim=0).squeeze(1)\n",
    "    print(mean_values.shape)\n",
    "    print(mean_values[0])\n",
    "    print(mean_values[-1])\n",
    "    # Calculate the ratio of the first timestep to the last timestep\n",
    "    ratio = mean_values[0] / mean_values[-1]\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "# Process train and test target sequences\n",
    "train_ratio = process_sequence(initial_train_target_sequence_diff)\n",
    "test_ratio = process_sequence(initial_test_target_sequence_diff)\n",
    "\n",
    "# Output the results (you might want to format or log these depending on your use case)\n",
    "print(\"Training Data:\")\n",
    "#print(\"Minimum Values:\", train_min)\n",
    "print(\"Ratio First to Last Timestep:\", train_ratio)\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "#print(\"Minimum Values:\", test_min)\n",
    "print(\"Ratio First to Last Timestep:\", test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall proceed to train the model with RMSE of linearly decreasing weights, where max_ratio_differ is 1\n",
    "\n",
    "It is much more costly to regress wrong the first flow curve incremental changes than the last ones, since if the first ones are wrong, these errors would accumulate and totally shift the flow curve. You can set max_ratio_differ to a higher value than 1 to prioritize the first points accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from optimization.LSTM_helper import *\n",
    "from optimization.LSTM import *\n",
    "\n",
    "def train_LSTM(previous_model, current_model, previous_best_test_loss, \n",
    "               dropout, num_epochs, start_lr, end_lr, start_tf, end_tf, \n",
    "               weight_decay, max_ratio_differ):\n",
    "    \n",
    "    train_dataset = TensorDataset(initial_train_source_sequence_diff, initial_train_target_sequence_diff)\n",
    "    test_dataset = TensorDataset(initial_test_source_sequence_diff, initial_test_target_sequence_diff)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Parameters\n",
    "    _, source_len, feature_size = initial_train_source_sequence_diff.shape\n",
    "    _, target_len, label_size = initial_train_target_sequence_diff.shape\n",
    "    \n",
    "    hidden_size = 256\n",
    "    num_layers = 3\n",
    "    \n",
    "    bidirectional = True  # Set this flag to True or False as needed\n",
    "    use_attention = True  # Set this flag to True or False to enable/disable attention\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LSTMModel(feature_size, label_size,\n",
    "                      source_len, target_len,\n",
    "                      hidden_size, num_layers,\n",
    "                      dropout=dropout,\n",
    "                      bidirectional=bidirectional, \n",
    "                      use_attention=use_attention).to(device)\n",
    "    \n",
    "    criterion = RMSELoss(linear_weight=True, max_ratio_differ = max_ratio_differ)  # Use the custom RMSE loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=start_lr, weight_decay=weight_decay)  # Adding L2 regularization\n",
    "\n",
    "    if previous_model is not None:\n",
    "        # Loading the best model from the previous training\n",
    "        model.load_state_dict(torch.load(f\"{models_path}/LSTM/initial/best_model_{previous_model}.pth\"))\n",
    "        best_test_loss = previous_best_test_loss\n",
    "    else:\n",
    "        best_test_loss = float('inf')\n",
    "        \n",
    "    # Count the number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'The model has {total_params} parameters.')\n",
    "    \n",
    "    # Track the best model\n",
    "    best_model_path = f\"{models_path}/LSTM/initial/best_model_{current_model}.pth\"\n",
    "    \n",
    "    # Lists to track train and test losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "    \n",
    "        # Update learning rate\n",
    "        current_lr = linear_lr_scheduler(optimizer, epoch, start_lr, end_lr, num_epochs)\n",
    "        \n",
    "        # Get the current teacher forcing probability from the scheduler\n",
    "        teacher_forcing_prob = linear_teacher_forcing_scheduler(epoch, start_tf, end_tf, num_epochs)\n",
    "    \n",
    "        # teacher_forcing_prob = log_teacher_forcing_scheduler(epoch, start_tf, end_tf, num_epochs)\n",
    "        for batch_idx, (source_batch, target_batch) in enumerate(train_loader):\n",
    "            source_batch, target_batch = source_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(source_batch, target_batch, teacher_forcing_prob)\n",
    "            loss = criterion(outputs, target_batch)\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for source_batch, target_batch in test_loader:\n",
    "                source_batch, target_batch = source_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(source_batch)\n",
    "                loss = criterion(outputs, target_batch)\n",
    "    \n",
    "                test_loss += loss.item()\n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "        # Save the best model\n",
    "        if test_loss < best_test_loss:\n",
    "            print(f\"New best test loss found: {test_loss}\")\n",
    "            best_test_loss = test_loss\n",
    "            best_model = model\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "        # Print progress\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, LR: {current_lr:.9f}, TF: {teacher_forcing_prob:.9f}')\n",
    "    \n",
    "    # Save the train and test loss lists as .npy files\n",
    "    np.save(f'{models_path}/LSTM/initial/train_losses_{current_model}.npy', np.array(train_losses))\n",
    "    np.save(f'{models_path}/LSTM/initial/test_losses_{current_model}.npy', np.array(test_losses))\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"{models_path}/LSTM/initial/last_model_{current_model}.pth\")\n",
    "    \n",
    "    print('Training complete')\n",
    "    print(f'Best model saved with test loss: {best_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8955393 parameters.\n",
      "New best test loss found: 5.5944013595581055\n",
      "New best test loss found: 5.592984199523926\n",
      "New best test loss found: 5.58970832824707\n",
      "New best test loss found: 5.586631774902344\n",
      "New best test loss found: 5.584264278411865\n",
      "New best test loss found: 5.583493709564209\n",
      "New best test loss found: 5.581272125244141\n",
      "New best test loss found: 5.58078670501709\n",
      "New best test loss found: 5.577939987182617\n",
      "New best test loss found: 5.576466083526611\n",
      "New best test loss found: 5.569815158843994\n",
      "New best test loss found: 5.5618977546691895\n",
      "New best test loss found: 5.537265300750732\n",
      "New best test loss found: 5.362679481506348\n",
      "New best test loss found: 5.159878730773926\n",
      "New best test loss found: 4.84882116317749\n",
      "New best test loss found: 4.590609550476074\n",
      "New best test loss found: 4.537997722625732\n",
      "New best test loss found: 4.468242645263672\n",
      "New best test loss found: 4.419610023498535\n",
      "New best test loss found: 4.319255828857422\n",
      "New best test loss found: 4.2385334968566895\n",
      "New best test loss found: 4.171587944030762\n",
      "New best test loss found: 4.128103256225586\n",
      "New best test loss found: 4.059876441955566\n",
      "New best test loss found: 4.018707752227783\n",
      "New best test loss found: 3.9489588737487793\n",
      "New best test loss found: 3.930783748626709\n",
      "New best test loss found: 3.916388750076294\n",
      "New best test loss found: 3.8798885345458984\n",
      "New best test loss found: 3.8164727687835693\n",
      "New best test loss found: 3.788882255554199\n",
      "New best test loss found: 3.767446756362915\n",
      "New best test loss found: 3.6699330806732178\n",
      "New best test loss found: 3.568246364593506\n",
      "New best test loss found: 3.524132013320923\n",
      "New best test loss found: 3.46451997756958\n",
      "New best test loss found: 3.4242801666259766\n",
      "New best test loss found: 3.3876237869262695\n",
      "New best test loss found: 3.3698856830596924\n",
      "New best test loss found: 3.3639698028564453\n",
      "New best test loss found: 3.304126024246216\n",
      "New best test loss found: 3.202317953109741\n",
      "New best test loss found: 3.159888982772827\n",
      "New best test loss found: 3.039309501647949\n",
      "New best test loss found: 2.9522182941436768\n",
      "New best test loss found: 2.8172192573547363\n",
      "New best test loss found: 2.762611150741577\n",
      "New best test loss found: 2.6355836391448975\n",
      "New best test loss found: 2.580249547958374\n",
      "New best test loss found: 2.541930913925171\n",
      "New best test loss found: 2.4674088954925537\n",
      "New best test loss found: 2.463083267211914\n",
      "New best test loss found: 2.4097073078155518\n",
      "New best test loss found: 2.2718443870544434\n",
      "New best test loss found: 2.264770269393921\n",
      "New best test loss found: 2.228719472885132\n",
      "New best test loss found: 2.1385180950164795\n",
      "New best test loss found: 2.1002695560455322\n",
      "New best test loss found: 2.013305187225342\n",
      "Epoch [100/1000], Train Loss: 2.051567, Test Loss: 2.013305, LR: 0.000460400, TF: 0.901000000\n",
      "New best test loss found: 1.9794515371322632\n",
      "New best test loss found: 1.9687542915344238\n",
      "New best test loss found: 1.9102811813354492\n",
      "New best test loss found: 1.8870677947998047\n",
      "New best test loss found: 1.8677632808685303\n",
      "New best test loss found: 1.8341220617294312\n",
      "New best test loss found: 1.8129948377609253\n",
      "New best test loss found: 1.7964324951171875\n",
      "New best test loss found: 1.7190970182418823\n",
      "New best test loss found: 1.7173322439193726\n",
      "New best test loss found: 1.6923303604125977\n",
      "New best test loss found: 1.6314462423324585\n",
      "New best test loss found: 1.5928610563278198\n",
      "New best test loss found: 1.5737133026123047\n",
      "New best test loss found: 1.535378336906433\n",
      "New best test loss found: 1.5237171649932861\n",
      "New best test loss found: 1.4565551280975342\n",
      "New best test loss found: 1.4300029277801514\n",
      "New best test loss found: 1.4112310409545898\n",
      "New best test loss found: 1.3997818231582642\n",
      "New best test loss found: 1.372040867805481\n",
      "New best test loss found: 1.3411896228790283\n",
      "New best test loss found: 1.3279204368591309\n",
      "New best test loss found: 1.3169853687286377\n",
      "New best test loss found: 1.3090102672576904\n",
      "New best test loss found: 1.298354148864746\n",
      "New best test loss found: 1.2627909183502197\n",
      "New best test loss found: 1.2497003078460693\n",
      "New best test loss found: 1.243254542350769\n",
      "New best test loss found: 1.227346420288086\n",
      "Epoch [200/1000], Train Loss: 1.354419, Test Loss: 1.365833, LR: 0.000420400, TF: 0.801000000\n",
      "New best test loss found: 1.2216086387634277\n",
      "New best test loss found: 1.2057067155838013\n",
      "New best test loss found: 1.184760332107544\n",
      "New best test loss found: 1.1649202108383179\n",
      "New best test loss found: 1.1624115705490112\n",
      "New best test loss found: 1.1567165851593018\n",
      "New best test loss found: 1.1481616497039795\n",
      "New best test loss found: 1.1100735664367676\n",
      "New best test loss found: 1.0974868535995483\n",
      "New best test loss found: 1.0865696668624878\n",
      "New best test loss found: 1.079666256904602\n",
      "New best test loss found: 1.0744234323501587\n",
      "New best test loss found: 1.0592131614685059\n",
      "New best test loss found: 1.054436206817627\n",
      "New best test loss found: 1.0431026220321655\n",
      "New best test loss found: 1.0286861658096313\n",
      "New best test loss found: 1.0103578567504883\n",
      "New best test loss found: 1.0090309381484985\n",
      "New best test loss found: 1.0065999031066895\n",
      "New best test loss found: 1.003000020980835\n",
      "New best test loss found: 0.9924113154411316\n",
      "New best test loss found: 0.9820361137390137\n",
      "Epoch [300/1000], Train Loss: 1.019052, Test Loss: 1.024784, LR: 0.000380400, TF: 0.701000000\n",
      "New best test loss found: 0.9587113857269287\n",
      "New best test loss found: 0.9366393685340881\n",
      "New best test loss found: 0.9168791174888611\n",
      "New best test loss found: 0.9045923352241516\n",
      "New best test loss found: 0.8925352096557617\n",
      "New best test loss found: 0.8856339454650879\n",
      "New best test loss found: 0.866328239440918\n",
      "New best test loss found: 0.8622074723243713\n",
      "New best test loss found: 0.85551518201828\n",
      "New best test loss found: 0.855154275894165\n",
      "New best test loss found: 0.8427013754844666\n",
      "New best test loss found: 0.8423661589622498\n",
      "New best test loss found: 0.8344822525978088\n",
      "New best test loss found: 0.8260896801948547\n",
      "New best test loss found: 0.8133463859558105\n",
      "New best test loss found: 0.8092007040977478\n",
      "New best test loss found: 0.7890446186065674\n",
      "New best test loss found: 0.7868852615356445\n",
      "New best test loss found: 0.7620471119880676\n",
      "New best test loss found: 0.756582498550415\n",
      "New best test loss found: 0.7481763362884521\n",
      "New best test loss found: 0.7407307624816895\n",
      "New best test loss found: 0.7332587242126465\n",
      "Epoch [400/1000], Train Loss: 0.771722, Test Loss: 0.828198, LR: 0.000340400, TF: 0.601000000\n",
      "New best test loss found: 0.7318357229232788\n",
      "New best test loss found: 0.7066707611083984\n",
      "New best test loss found: 0.690629780292511\n",
      "New best test loss found: 0.6819218993186951\n",
      "New best test loss found: 0.6692485213279724\n",
      "New best test loss found: 0.6691235303878784\n",
      "New best test loss found: 0.6576469540596008\n",
      "New best test loss found: 0.6378291845321655\n",
      "New best test loss found: 0.6337248086929321\n",
      "New best test loss found: 0.617070734500885\n",
      "New best test loss found: 0.6083436012268066\n",
      "New best test loss found: 0.6030555367469788\n",
      "New best test loss found: 0.600727915763855\n",
      "Epoch [500/1000], Train Loss: 0.646932, Test Loss: 0.646387, LR: 0.000300400, TF: 0.501000000\n",
      "New best test loss found: 0.5984413623809814\n",
      "New best test loss found: 0.5944775938987732\n",
      "New best test loss found: 0.5740607976913452\n",
      "New best test loss found: 0.5681792497634888\n",
      "New best test loss found: 0.5538910031318665\n",
      "New best test loss found: 0.5506051778793335\n",
      "New best test loss found: 0.5500039458274841\n",
      "New best test loss found: 0.5475587248802185\n",
      "New best test loss found: 0.5290005207061768\n",
      "New best test loss found: 0.5198390483856201\n",
      "New best test loss found: 0.5170254707336426\n",
      "Epoch [600/1000], Train Loss: 0.512755, Test Loss: 0.528028, LR: 0.000260400, TF: 0.401000000\n",
      "New best test loss found: 0.5038060545921326\n",
      "New best test loss found: 0.5017516613006592\n",
      "New best test loss found: 0.48927631974220276\n",
      "New best test loss found: 0.4849489629268646\n",
      "New best test loss found: 0.48126861453056335\n",
      "New best test loss found: 0.4764620363712311\n",
      "New best test loss found: 0.4726731777191162\n",
      "New best test loss found: 0.467938631772995\n",
      "Epoch [700/1000], Train Loss: 0.541206, Test Loss: 0.559832, LR: 0.000220400, TF: 0.301000000\n",
      "New best test loss found: 0.4625219404697418\n",
      "New best test loss found: 0.46202513575553894\n",
      "New best test loss found: 0.45914408564567566\n",
      "New best test loss found: 0.4564014971256256\n",
      "New best test loss found: 0.45334064960479736\n",
      "New best test loss found: 0.45110684633255005\n",
      "Epoch [800/1000], Train Loss: 0.441100, Test Loss: 0.469608, LR: 0.000180400, TF: 0.201000000\n",
      "New best test loss found: 0.44540610909461975\n",
      "New best test loss found: 0.44130417704582214\n",
      "New best test loss found: 0.44098222255706787\n",
      "New best test loss found: 0.43939533829689026\n",
      "New best test loss found: 0.43610242009162903\n",
      "New best test loss found: 0.4351905882358551\n",
      "Epoch [900/1000], Train Loss: 0.424788, Test Loss: 0.461655, LR: 0.000140400, TF: 0.101000000\n",
      "New best test loss found: 0.4305228590965271\n",
      "New best test loss found: 0.42947614192962646\n",
      "New best test loss found: 0.4270029067993164\n",
      "New best test loss found: 0.4238044023513794\n",
      "New best test loss found: 0.4221002757549286\n",
      "Epoch [1000/1000], Train Loss: 0.401421, Test Loss: 0.423918, LR: 0.000100400, TF: 0.001000000\n",
      "Training complete\n",
      "Best model saved with test loss: 0.4221\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(previous_model=None, current_model=\"1_train_diff\", previous_best_test_loss=None, \n",
    "               dropout=0.01, num_epochs=1000, start_lr=0.0005, end_lr=0.0001, start_tf=1.0, end_tf=0.0, \n",
    "               weight_decay=1e-3, max_ratio_differ=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8955393 parameters.\n",
      "New best test loss found: 0.420843243598938\n",
      "New best test loss found: 0.4193941652774811\n",
      "New best test loss found: 0.4193079471588135\n",
      "Epoch [100/1000], Train Loss: 0.382713, Test Loss: 0.419308, LR: 0.000091090, TF: 0.901000000\n",
      "New best test loss found: 0.4159759283065796\n",
      "New best test loss found: 0.4133967459201813\n",
      "Epoch [200/1000], Train Loss: 0.387355, Test Loss: 0.423981, LR: 0.000082090, TF: 0.801000000\n",
      "New best test loss found: 0.4114418029785156\n",
      "New best test loss found: 0.4112423360347748\n",
      "New best test loss found: 0.41071128845214844\n",
      "New best test loss found: 0.40998151898384094\n",
      "New best test loss found: 0.40996912121772766\n",
      "New best test loss found: 0.40948426723480225\n",
      "New best test loss found: 0.4079590439796448\n",
      "Epoch [300/1000], Train Loss: 0.373489, Test Loss: 0.421219, LR: 0.000073090, TF: 0.701000000\n",
      "New best test loss found: 0.40697377920150757\n",
      "New best test loss found: 0.40667974948883057\n",
      "New best test loss found: 0.40556126832962036\n",
      "New best test loss found: 0.40511614084243774\n",
      "Epoch [400/1000], Train Loss: 0.382504, Test Loss: 0.419840, LR: 0.000064090, TF: 0.601000000\n",
      "New best test loss found: 0.40404462814331055\n",
      "New best test loss found: 0.4037960171699524\n",
      "New best test loss found: 0.4028151333332062\n",
      "New best test loss found: 0.40189382433891296\n",
      "New best test loss found: 0.4016605615615845\n",
      "New best test loss found: 0.4013954699039459\n",
      "Epoch [500/1000], Train Loss: 0.362569, Test Loss: 0.411524, LR: 0.000055090, TF: 0.501000000\n",
      "New best test loss found: 0.39906278252601624\n",
      "Epoch [600/1000], Train Loss: 0.358789, Test Loss: 0.404014, LR: 0.000046090, TF: 0.401000000\n",
      "New best test loss found: 0.39818546175956726\n",
      "New best test loss found: 0.397530734539032\n",
      "Epoch [700/1000], Train Loss: 0.358504, Test Loss: 0.413313, LR: 0.000037090, TF: 0.301000000\n",
      "New best test loss found: 0.39682191610336304\n",
      "Epoch [800/1000], Train Loss: 0.352626, Test Loss: 0.398438, LR: 0.000028090, TF: 0.201000000\n",
      "New best test loss found: 0.3966546356678009\n",
      "New best test loss found: 0.39650994539260864\n",
      "New best test loss found: 0.39564549922943115\n",
      "Epoch [900/1000], Train Loss: 0.346131, Test Loss: 0.399216, LR: 0.000019090, TF: 0.101000000\n",
      "New best test loss found: 0.3954865336418152\n",
      "New best test loss found: 0.39512336254119873\n",
      "New best test loss found: 0.3948955833911896\n",
      "Epoch [1000/1000], Train Loss: 0.347126, Test Loss: 0.397785, LR: 0.000010090, TF: 0.001000000\n",
      "Training complete\n",
      "Best model saved with test loss: 0.3949\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(previous_model=\"1_train_diff\", current_model=\"2_train_diff\", previous_best_test_loss=0.4221002757549286, \n",
    "               dropout=0.01, num_epochs=1000, start_lr=0.0001, end_lr=0.00001, start_tf=1.0, end_tf=0.0, \n",
    "               weight_decay=1e-3, max_ratio_differ=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5092767,
     "sourceId": 8527979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
